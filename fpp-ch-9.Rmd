---
title: "fpp-ch-9"
output: html_document
date: "2023-02-02"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Load Packages

```{r, results='hide', message=FALSE}

install.packages("data.table")
install.packages("readr")
install.packages("USgas")
install.packages("readxl")
install.packages("ggdist")
install.packages("urca")

```

```{r}
library(data.table) 
library(fpp3)
library(readr)
library(USgas)
library(scales)
library(readxl)
library(ggdist)
#library(urca)

```

## Qn 2

The plots below show that there is high level of correlation between the previous level of the stock and the next level of the stock, indicating that we should take a first difference.

The slow drop in ACF is an indication that the series needs to be differenced.

A convenient way to produce a time plot, ACF plot and PACF plot in one command is to use the gg_tsdisplay() function with plot_type = "partial

```{r}

gafa_stock %>% distinct(Symbol)

gafa_stock

a_stock <- gafa_stock %>% 
  filter(Symbol == "AMZN")

a_stock %>% 
  ACF(Close) %>% 
  autoplot()

a_stock %>% 
  PACF(Close) %>% 
  autoplot()
```

# Qn 3

```{r}

global_economy %>% head()
global_economy %>% distinct(Country)

turkey_ec <- global_economy %>% 
  filter(Country == "Turkey")

# take first difference and plot it
turkey_ec %>% autoplot(GDP)
turkey_ec %>% autoplot(difference(GDP))

# apply the box_cox transform
turk_feat <- turkey_ec %>%
  features(GDP, features = guerrero)
(lambda <- turk_feat %>%
  pull(lambda_guerrero))

turkey_ec %>%
  autoplot(box_cox(GDP, lambda))

turkey_ec <- turkey_ec %>% 
  mutate(box_cox_gdp = box_cox(GDP, lambda)) %>% 
  mutate(diff_box_cox_gdp = difference(box_cox_gdp))

head(turkey_ec)

turkey_ec %>% autoplot(diff_box_cox_gdp)

# add do stationary test

turkey_ec %>% features(diff_box_cox_gdp, c(unitroot_kpss, unitroot_ndiffs))
turkey_ec %>% features(GDP, c(unitroot_kpss, unitroot_ndiffs))

```

```{r}

# some testing code to figure out the best way to pull the index and keys from a tsibble

key_vars(aus_accommodation)
index(aus_accommodation)

typeof(key(aus_accommodation)) # returns a list of symbols
typeof(key(aus_accommodation)[[1]]) # this is a symbol
typeof(index(aus_accommodation)) # this is a symbol

test <- c(key(aus_accommodation), index(aus_accommodation))

# actual code

(tas_ac <- aus_accommodation %>% 
  filter(State == "Tasmania") %>% 
  select( index(aus_accommodation), all_of(key_vars(aus_accommodation)), Takings) # is there a better way to select the key?
)

tas_ac %>% autoplot(Takings)

# this requires a seasonal difference

tas_ac <- tas_ac %>% 
  mutate(s_diff_takings = difference(Takings, 12))

tas_ac %>% autoplot(s_diff_takings)

tas_ac %>% features(s_diff_takings, c(unitroot_kpss, unitroot_ndiffs))

# the p-value is > .1 so there is no evidence of a non-stationary series

```

```{r}

souvenirs %>% autoplot(Sales)

souvenirs <- souvenirs %>% mutate(
  log_sales = log(Sales),
  s_diff_sales = difference(Sales, 12),
  diff_diff_sales = difference(s_diff_sales)
)

souvenirs %>% autoplot(log_sales)
souvenirs %>% autoplot(s_diff_sales)
souvenirs %>% autoplot(diff_diff_sales)

souvenirs %>% features(log_sales, c(unitroot_kpss, unitroot_ndiffs, unitroot_nsdiffs))
souvenirs %>% features(s_diff_sales, c(unitroot_kpss, unitroot_ndiffs, unitroot_nsdiffs))
souvenirs %>% features(diff_diff_sales, c(unitroot_kpss, unitroot_ndiffs, unitroot_nsdiffs))


# based on the tests, we need to first take a seasonal difference then a first differece

```

# Qn 4 and 5

Skipping because I get the point

# Qn 6

A higher level for phi means that the series tends to look less random and stay above or below 0 for an extended period of time by the time phi gets to 1, then it's a random walk

Note that the PACF plot is sometimes used to determine the AR level. That's how PACF is measured, the correlation between two lags, removing the correlation caused by the intermediate lags.

We can see in the PACF and ACF plots that correlation between lags is high, but only as a side effect of the correlation between the first lag being very very high.

The motivation behind an AR model is that sometimes past values of a variable gives you information to help you predict the future value!!

```{r}

phi = 1

y <- numeric(100)
e <- rnorm(100)
for(i in 2:100)
  y[i] <- phi*y[i-1] + e[i]
sim <- tsibble(idx = seq_len(100), y = y, index = idx)

sim %>% autoplot(y)

sim %>% gg_tsdisplay(y, plot_type = "partial") 

```

## Qn 6c

The MA model means that past innovations affect future values of the variable.

Notice that each value of can be thought of as a weighted moving average of the past few forecast errors

Note that the ACF plot can sometimes be used to determine the MA level. ACF is calculated as just the auto correlation between different lags. The PACF is showing a sinusoidal pattern because the moment you drop the error from 1 period ago, it generates a "negative" partial autocorrelation

Finally, the motivation behind having an MA model is that you are averaging out a few of the past errors (almost like ETS) to help you make a prediction for what the future error will be.

MA models are called "Short Memory" models because the shocks get forgotten. But AR models are called "Long Memory" models because the shocks stay in the past variable which gets expoentially decayed

```{r}

theta = 1
number_iterations = 300

y <- numeric(number_iterations)
e <- rnorm(number_iterations)
for(i in 2:number_iterations)
  y[i] <- e[i] + theta*e[i-1]
sim <- tsibble(idx = seq_len(number_iterations), y = y, index = idx)

sim %>% autoplot(y)

sim %>% gg_tsdisplay(y, plot_type = "partial") 

```

```{r}
phi = 0.6
theta = 0.6
number_iterations = 100

y <- numeric(number_iterations)
e <- rnorm(number_iterations)
for(i in 2:number_iterations)
  y[i] <- phi*y[i-1] + e[i] + theta*e[i-1] # there must be a vectorized implementation
sim <- tsibble(idx = seq_len(number_iterations), y = y, index = idx)

sim %>% autoplot(y)


```

The non-stationary series oscillates back and forth very frequently

```{r}

phi = -0.8
theta = 0.3
number_iterations = 100

y <- numeric(number_iterations)
e <- rnorm(number_iterations)
for(i in 2:number_iterations)
  y[i] <- phi*y[i-1] + e[i] + theta*e[i-1] # there must be a vectorized implementation
sim <- tsibble(idx = seq_len(number_iterations), y = y, index = idx)

sim %>% autoplot(y)

```

# Qn 7
We got an ARIMA(0, 2, 1) model without logging it
We get a ARIMA(0, 1, 0) with drift when we log it

Maybe it'd be good to log the model because the residual variance looks a lot higher in the later years, and the residuals are right skewed.

```{r}
aus_airpassengers %>% autoplot(Passengers)
aus_airpassengers %>% autoplot(log(Passengers))

model_obj <- aus_airpassengers %>% 
  model(ARIMA(log(Passengers)))
  
report(model_obj)

model_obj %>% gg_tsresiduals()
  
fc <- model_obj %>% forecast(h = "10 years")

fc %>% autoplot(aus_airpassengers)

```

```{r}
aus_pax <- aus_airpassengers %>% 
  mutate(log_pax = log(Passengers))

model_obj <- aus_pax %>% 
  model(
    arima021 = ARIMA(Passengers ~ pdq(0, 2, 1) + 0),
    arima010 = ARIMA(Passengers ~ pdq(0, 1, 0) + 1), 
    arima212 = ARIMA(Passengers ~ pdq(2, 1, 2) + 1),
    #arima212noc = ARIMA(Passengers ~ pdq(2, 1, 2) + 0),
  )

model_obj

fc <- model_obj %>% 
  forecast(h = "10 years")

fc %>% filter(.model %in% c("arima021", "arima010", "arima212")) %>% 
  autoplot(aus_pax, level = NULL)

fc %>% filter(.model %in% c("arima021", "arima010", "arima212")) %>% 
  autoplot(level = NULL)

```

Hmm, I'm not really sure if I'm supposed to be comparing the logged or non-logged versions of the models and that's a bit confusing to me.

To answer qn D, when we take out the constant from the ARIMA(2, 1, 2) model, the model produces an error indicating "non-stationary AR part from CSS", which I think has to do with the coefficients falling outside the bounds, see the following link:
https://stackoverflow.com/questions/7233288/non-stationary-seasonal-ar-part-from-css-error-in-r

The model that is twice differences grows much faster than the other models
The model that has the 2 AR terms tend to have forecasts that oscillates up and down compared to the simple MA model. This is because the MA model forecast has to assume that future errors are 0.

## Qn 7c
When we add the constant, the model warns us that this is not a safe operation. The resulting model blows up very quickly
```{r}

model_obj <- aus_pax %>% 
  model(
    arima021 = ARIMA(Passengers ~ pdq(0, 2, 1) + 0),
    arima021poly = ARIMA(Passengers ~ pdq(0, 2, 1) + 1)
  )

fc <- model_obj %>% 
  forecast(h = "10 years")

fc %>% autoplot(aus_pax, level=NULL)

```

# Qn 8

Residuals are slightly negative skewed.

ETS model does not grow as much as the ARIMA. The AICc is a lot worse, though I don't think it's comparable.

```{r}
us_gdp <- global_economy %>% 
  filter(Country == "United States") %>% 
  select(Country, Year, GDP)

us_gdp %>% autoplot(GDP)

lambda <- us_gdp %>%
  features(GDP, guerrero) %>% 
  pull(lambda_guerrero)

model_obj <- us_gdp %>% 
  model(
    auto_model = ARIMA(box_cox(GDP, lambda)),
    arima111 = ARIMA(box_cox(GDP, lambda) ~ pdq(1, 1, 1)),
    full_search = ARIMA(box_cox(GDP, lambda), stepwise=FALSE),
    ETS = ETS(GDP)
)

model_obj

model_obj %>% 
  select(auto_model) %>% 
  report()

model_obj %>% 
  select(ETS) %>% 
  report()

selected_model <-model_obj %>% 
  select(auto_model)

selected_model

selected_model %>% gg_tsresiduals()

us_gdp_aug <- selected_model %>% 
  augment()

us_gdp_aug %>% autoplot(GDP) +
  autolayer(us_gdp_aug, .fitted)

fc_obj <- model_obj %>% 
  forecast(h = "10 years")

fc_obj %>% filter(.model %in% c("auto_model", "ETS"))

fc_obj %>% filter(.model %in% c("auto_model", "ETS")) %>% 
  autoplot(us_gdp, level=NULL)

```

# Qn 9

The time plot is seasonal throughout the year. The seasonal variations are proportional to size of the plot

```{r}
aus_arrivals

aus_arrivals %>%  distinct(Origin)

aus_jap <- aus_arrivals %>% 
  filter(Origin == "US")

aus_jap %>% autoplot(Arrivals)

aus_jap_transform <- aus_jap %>% 
  transmute(
    arr = Arrivals,
    log_arr = log(Arrivals),
    s_arr = difference(log(Arrivals), 12),
    d_s_arr = difference(s_arr)
  ) 

aus_jap_transform  %>% 
  pivot_longer(-Quarter, names_to = "Type", values_to = "Trans_Value") %>% 
  mutate(Type = factor(Type, levels = c(
    "arr",
    "log_arr",
    "s_arr",
    "d_s_arr" ))
  ) %>% 
  ggplot(aes(x=Quarter, y=Trans_Value)) +
  geom_line() +
  facet_grid(vars(Type), scales = "free_y")

```
Run the unit root tests. This indicates that we want to log, sdiff, and diff

```{r}

aus_jap_transform %>% 
  features(log_arr, unitroot_nsdiffs)

aus_jap_transform %>% 
  features(s_arr, unitroot_ndiffs)

aus_jap_transform %>% 
  features(d_s_arr, unitroot_ndiffs)

```

Look at the ACF/PACF Graphs. 

The PACF has significant auto-correlations at lag 4, indicating a seasonal ARIMA would be appropriate

Because of the significant lags at PACF2, we will choose an AR(2), with one seasonal difference and one first difference

```{r}

aus_jap_transform %>% gg_tsdisplay(d_s_arr, plot_type = "partial") 

```
We will try both a ARIMA(2,1,0)(1,1,0) and the automatic model. We see the automatic model has a much lower RMSE. Ideally we would look at the out of sample tests for this, using some testing data. but I don't feel like doing this right now

```{r}

# aus_jap

arr_model <- aus_jap  %>% 
  model(
    arima210011 = ARIMA(log(Arrivals) ~ pdq(2, 1, 0) + PDQ(1, 1, 0)),
    arima_auto = ARIMA(log(Arrivals))
  )

arr_model %>% select(arima210011) %>% report()
arr_model %>% select(arima_auto) %>% report()

arr_model %>% accuracy()

```
we see the automatic model does not predict as much seasonal variation. 

```{r}

arr_fc <- arr_model %>% 
  forecast(h = 10)

arr_fc %>% autoplot(aus_jap, level = NULL) 

```
Comparing residuals of the two models, the automatic model is much better:
* there are not as many significant ACFs
* the residual plot does not look as skewed

```{r}

arr_model %>% 
  select(arima_auto) %>% 
  gg_tsresiduals()

arr_model %>% 
  select(arima210011) %>% 
  gg_tsresiduals()

```


# Qn 10 

## 10a

Produce an STL decomposition of the data and describe the trend and seasonality.

```{r}
# Cmd + Option + I creates a new chunk
# on mac, Cmd + Shift + M gives you the pipe operater

us_employment %>% distinct(Title)

us_tot_private <- us_employment %>% 
  filter(Title == "Total Private")

us_tot_private %>% autoplot(Employed)

us_emp_model <- us_tot_private |> 
  model(
    STL(Employed)
  )

us_emp_model |> 
  components() |> 
  autoplot()


```

## 10b

Do the data need transforming? If so, find a suitable transformation.

Yes, the data requires transforming. We can see that the seasonal pattern is increasing over time, indicating that we should do a log or box_cox transform.

```{r}
(us_feat <- us_tot_private %>%
  features(Employed, features = guerrero))
(lambda <- us_feat %>%
  pull(lambda_guerrero))

us_emp_model <- us_tot_private |> 
  model(
    STL(box_cox(Employed, lambda))
  )

us_emp_model |> 
  components() |> 
  autoplot()

```

## 10c
Check the seasonality - from STL decomposition - which months have the highest and lowest? Do that gg plot that has 12 plots (each month) and shows within each month, employment by month.
  Seasonality is not a big component compared to the level, but summers tend to have higher employment, which makes sense due to summer seasonal work

Are the data stationary? If not, find an appropriate differencing which yields stationary data.

```{r}

us_tot_private <- us_tot_private |> 
  mutate(box_emp = box_cox(Employed, lambda)) 

us_tot_private |> 
  gg_subseries(Employed)

us_tot_private |> features(box_emp, c(unitroot_kpss, unitroot_ndiffs)) # the data is not stationary (as we can also tell visually)

us_tot_private |> features(difference(box_emp, lag = 12), c(unitroot_kpss, unitroot_ndiffs)) # after taking the first seasonal differene, we don't need another difference.

```
Let's plot out the transformations that we did.

```{r}
us_tot_trans <- us_tot_private %>% 
  transmute(
    emp = Employed,
    box_emp = box_cox(Employed, lambda),
    log_emp = log(Employed),
    s_diff_box_emp = difference(box_emp, 12),
  )

us_tot_trans  %>% 
  pivot_longer(-Month, names_to = "Type", values_to = "Trans_Value") %>% 
  mutate(Type = factor(Type, levels = c(
    "emp",
    "box_emp",
    "log_emp",
    "s_diff_box_emp" ))
  ) %>% 
  ggplot(aes(x=Month, y=Trans_Value)) +
  geom_line() +
  facet_grid(vars(Type), scales = "free_y")
```

## 10d

Identify a couple of ARIMA models that might be useful in describing the time series. Which of your models is the best according to their AICc values?

Estimate the parameters of your best model and do diagnostic testing on the residuals. Do the residuals resemble white noise? If not, try to find another ARIMA model which fits better.
Forecast the next 3 years of data. Get the latest figures from https://fred.stlouisfed.org/categories/11 to check the accuracy of your forecasts.
Eventually, the prediction intervals are so wide that the forecasts are not particularly useful. How many years of forecasts do you think are sufficiently accurate to be usable?


# 11 and 12

This is interesting because it conmpares seasonal model to non-seasonal model on seasonally adjusted STL data

# 13

This is interesting because you're fitting the model for all series

# 14 - Skip

# 15

This is interesting because of hand calculating the forecasts

# 16

Identify the series

# 17 

Working with Quandl








