---
title: "fpp-ch-9"
output: html_document
date: "2023-02-02"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, results='hide', message=FALSE}

install.packages("data.table")
install.packages("readr")
install.packages("USgas")
install.packages("readxl")
install.packages("ggdist")
#install.packages("urca")

```

```{r}
library(data.table) 
library(fpp3)
library(readr)
library(USgas)
library(scales)
library(readxl)
library(ggdist)
#library(urca)

```

## Qn 2

The plots below show that there is high level of correlation between the previous level of the stock and the next level of the stock, indicating that we should take a first difference.

The slow drop in ACF is an indication that the series needs to be differenced.

A convenient way to produce a time plot, ACF plot and PACF plot in one command is to use the gg_tsdisplay() function with plot_type = "partial

```{r}

gafa_stock %>% distinct(Symbol)

gafa_stock

a_stock <- gafa_stock %>% 
  filter(Symbol == "AMZN")

a_stock %>% 
  ACF(Close) %>% 
  autoplot()

a_stock %>% 
  PACF(Close) %>% 
  autoplot()
```

# Qn 3

```{r}

global_economy %>% head()
global_economy %>% distinct(Country)

turkey_ec <- global_economy %>% 
  filter(Country == "Turkey")

# take first difference and plot it
turkey_ec %>% autoplot(GDP)
turkey_ec %>% autoplot(difference(GDP))

# apply the box_cox transform
turk_feat <- turkey_ec %>%
  features(GDP, features = guerrero)
(lambda <- turk_feat %>%
  pull(lambda_guerrero))

turkey_ec %>%
  autoplot(box_cox(GDP, lambda))

turkey_ec <- turkey_ec %>% 
  mutate(box_cox_gdp = box_cox(GDP, lambda)) %>% 
  mutate(diff_box_cox_gdp = difference(box_cox_gdp))

head(turkey_ec)

turkey_ec %>% autoplot(diff_box_cox_gdp)

# add do stationary test

turkey_ec %>% features(diff_box_cox_gdp, c(unitroot_kpss, unitroot_ndiffs))
turkey_ec %>% features(GDP, c(unitroot_kpss, unitroot_ndiffs))

```

```{r}

# some testing code to figure out the best way to pull the index and keys from a tsibble

key_vars(aus_accommodation)
index(aus_accommodation)

typeof(key(aus_accommodation)) # returns a list of symbols
typeof(key(aus_accommodation)[[1]]) # this is a symbol
typeof(index(aus_accommodation)) # this is a symbol

test <- c(key(aus_accommodation), index(aus_accommodation))

# actual code

(tas_ac <- aus_accommodation %>% 
  filter(State == "Tasmania") %>% 
  select( index(aus_accommodation), all_of(key_vars(aus_accommodation)), Takings) # is there a better way to select the key?
)

tas_ac %>% autoplot(Takings)

# this requires a seasonal difference

tas_ac <- tas_ac %>% 
  mutate(s_diff_takings = difference(Takings, 12))

tas_ac %>% autoplot(s_diff_takings)

tas_ac %>% features(s_diff_takings, c(unitroot_kpss, unitroot_ndiffs))

# the p-value is > .1 so there is no evidence of a non-stationary series

```

```{r}

souvenirs %>% autoplot(Sales)

souvenirs <- souvenirs %>% mutate(
  log_sales = log(Sales),
  s_diff_sales = difference(Sales, 12),
  diff_diff_sales = difference(s_diff_sales)
)

souvenirs %>% autoplot(log_sales)
souvenirs %>% autoplot(s_diff_sales)
souvenirs %>% autoplot(diff_diff_sales)

souvenirs %>% features(log_sales, c(unitroot_kpss, unitroot_ndiffs, unitroot_nsdiffs))
souvenirs %>% features(s_diff_sales, c(unitroot_kpss, unitroot_ndiffs, unitroot_nsdiffs))
souvenirs %>% features(diff_diff_sales, c(unitroot_kpss, unitroot_ndiffs, unitroot_nsdiffs))


# based on the tests, we need to first take a seasonal difference then a first differece

```

# Qn 4 and 5

Skipping because I get the point

# Qn 6

A higher level for phi means that the series tends to look less random and stay above or below 0 for an extended period of time by the time phi gets to 1, then it's a random walk

Note that the PACF plot is sometimes used to determine the AR level. That's how PACF is measured, the correlation between two lags, removing the correlation caused by the intermediate lags.

We can see in the PACF and ACF plots that correlation between lags is high, but only as a side effect of the correlation between the first lag being very very high.

The motivation behind an AR model is that sometimes past values of a variable gives you information to help you predict the future value!!

```{r}

phi = 1

y <- numeric(100)
e <- rnorm(100)
for(i in 2:100)
  y[i] <- phi*y[i-1] + e[i]
sim <- tsibble(idx = seq_len(100), y = y, index = idx)

sim %>% autoplot(y)

sim %>% gg_tsdisplay(y, plot_type = "partial") 

```

## Qn 6c

The MA model means that past innovations affect future values of the variable.

Notice that each value of can be thought of as a weighted moving average of the past few forecast errors

Note that the ACF plot can sometimes be used to determine the MA level. ACF is calculated as just the auto correlation between different lags. The PACF is showing a sinusoidal pattern because the moment you drop the error from 1 period ago, it generates a "negative" partial autocorrelation

Finally, the motivation behind having an MA model is that you are averaging out a few of the past errors (almost like ETS) to help you make a prediction for what the future error will be.

MA models are called "Short Memory" models because the shocks get forgotten. But AR models are called "Long Memory" models because the shocks stay in the past variable which gets expoentially decayed

```{r}

theta = 1
number_iterations = 300

y <- numeric(number_iterations)
e <- rnorm(number_iterations)
for(i in 2:number_iterations)
  y[i] <- e[i] + theta*e[i-1]
sim <- tsibble(idx = seq_len(number_iterations), y = y, index = idx)

sim %>% autoplot(y)

sim %>% gg_tsdisplay(y, plot_type = "partial") 

```

```{r}
phi = 0.6
theta = 0.6
number_iterations = 100

y <- numeric(number_iterations)
e <- rnorm(number_iterations)
for(i in 2:number_iterations)
  y[i] <- phi*y[i-1] + e[i] + theta*e[i-1] # there must be a vectorized implementation
sim <- tsibble(idx = seq_len(number_iterations), y = y, index = idx)

sim %>% autoplot(y)


```

The non-stationary series oscillates back and forth very frequently

```{r}

phi = -0.8
theta = 0.3
number_iterations = 100

y <- numeric(number_iterations)
e <- rnorm(number_iterations)
for(i in 2:number_iterations)
  y[i] <- phi*y[i-1] + e[i] + theta*e[i-1] # there must be a vectorized implementation
sim <- tsibble(idx = seq_len(number_iterations), y = y, index = idx)

sim %>% autoplot(y)

```

# Qn 7
We got an ARIMA(0, 2, 1) model without logging it
We get a ARIMA(0, 1, 0) with drift when we log it

Maybe it'd be good to log the model because the residual variance looks a lot higher in the later years, and the residuals are right skewed.

```{r}
aus_airpassengers %>% autoplot(Passengers)
aus_airpassengers %>% autoplot(log(Passengers))

model_obj <- aus_airpassengers %>% 
  model(ARIMA(log(Passengers)))
  
report(model_obj)

model_obj %>% gg_tsresiduals()
  
fc <- model_obj %>% forecast(h = "10 years")

fc %>% autoplot(aus_airpassengers)

```

```{r}
aus_pax <- aus_airpassengers %>% 
  mutate(log_pax = log(Passengers))

model_obj <- aus_pax %>% 
  model(
    arima021 = ARIMA(Passengers ~ pdq(0, 2, 1) + 0),
    arima010 = ARIMA(Passengers ~ pdq(0, 1, 0) + 1), 
    arima212 = ARIMA(Passengers ~ pdq(2, 1, 2) + 1),
    #arima212noc = ARIMA(Passengers ~ pdq(2, 1, 2) + 0),
  )

model_obj

fc <- model_obj %>% 
  forecast(h = "10 years")

fc %>% filter(.model %in% c("arima021", "arima010", "arima212")) %>% 
  autoplot(aus_pax, level = NULL)

fc %>% filter(.model %in% c("arima021", "arima010", "arima212")) %>% 
  autoplot(level = NULL)

```

Hmm, I'm not really sure if I'm supposed to be comparing the logged or non-logged versions of the models and that's a bit confusing to me.

To answer qn D, when we take out the constant from the ARIMA(2, 1, 2) model, the model produces an error indicating "non-stationary AR part from CSS", which I think has to do with the coefficients falling outside the bounds, see the following link:
https://stackoverflow.com/questions/7233288/non-stationary-seasonal-ar-part-from-css-error-in-r

The model that is twice differences grows much faster than the other models
The model that has the 2 AR terms tend to have forecasts that oscillates up and down compared to the simple MA model. This is because the MA model forecast has to assume that future errors are 0.

## Qn 7c
When we add the constant, the model warns us that this is not a safe operation. The resulting model blows up very quickly
```{r}

model_obj <- aus_pax %>% 
  model(
    arima021 = ARIMA(Passengers ~ pdq(0, 2, 1) + 0),
    arima021poly = ARIMA(Passengers ~ pdq(0, 2, 1) + 1)
  )

fc <- model_obj %>% 
  forecast(h = "10 years")

fc %>% autoplot(aus_pax, level=NULL)

```

# Qn 8

Residuals are slightly negative skewed.

ETS model does not grow as much as the ARIMA. The AICc is a lot worse, though I don't think it's comparable.

```{r}
us_gdp <- global_economy %>% 
  filter(Country == "United States") %>% 
  select(Country, Year, GDP)

us_gdp %>% autoplot(GDP)

lambda <- us_gdp %>%
  features(GDP, guerrero) %>% 
  pull(lambda_guerrero)

model_obj <- us_gdp %>% 
  model(
    auto_model = ARIMA(box_cox(GDP, lambda)),
    arima111 = ARIMA(box_cox(GDP, lambda) ~ pdq(1, 1, 1)),
    full_search = ARIMA(box_cox(GDP, lambda), stepwise=FALSE),
    ETS = ETS(GDP)
)

model_obj

model_obj %>% 
  select(auto_model) %>% 
  report()

model_obj %>% 
  select(ETS) %>% 
  report()

selected_model <-model_obj %>% 
  select(auto_model)

selected_model

selected_model %>% gg_tsresiduals()

us_gdp_aug <- selected_model %>% 
  augment()

us_gdp_aug %>% autoplot(GDP) +
  autolayer(us_gdp_aug, .fitted)

fc_obj <- model_obj %>% 
  forecast(h = "10 years")

fc_obj %>% filter(.model %in% c("auto_model", "ETS"))

fc_obj %>% filter(.model %in% c("auto_model", "ETS")) %>% 
  autoplot(us_gdp, level=NULL)

```

