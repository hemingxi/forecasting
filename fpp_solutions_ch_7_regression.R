# -*- coding: utf-8 -*-
"""fpp-solutions-ch-7.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cvq3rWABWm97sYhwV3HzcWLHCgOQhT9Z
"""

install.packages('fpp3')
install.packages('USgas')

library(data.table) 
library(fpp3)
library(readr)
library(USgas)
library(scales)
library(readxl)

"""# Qn 1

Half-hourly electricity demand for Victoria, Australia is contained in vic_elec. Extract the January 2014 electricity demand, and aggregate this data to daily with daily total demands and maximum temperatures.

## a
Plot the data and find the regression model for Demand with temperature as a predictor variable. Why is there a positive relationship?

## b
Produce a residual plot. Is the model adequate? Are there any outliers or influential observations?

## c
Use the model to forecast the electricity demand that you would expect for the next day if the maximum temperature was 15∘C and compare it with the forecast if the with maximum temperature was 35∘C
Do you believe these forecasts? The following R code will get you started:

## d
Give prediction intervals for your forecasts.

## e
Plot Demand vs Temperature for all of the available data in vic_elec aggregated to daily total demand and maximum temperature. What does this say about your model?
"""

head(vic_elec)

jan14_vic_elec <- vic_elec %>%
  filter(yearmonth(Time) == yearmonth("2014 Jan")) %>%
  index_by(Date = as_date(Time)) %>%
  summarise(
    Demand = sum(Demand),
    Temperature = max(Temperature)
  )

head(jan14_vic_elec)

# Qn 1a

jan14_vic_elec %>%
  ggplot(aes(x = Temperature, y = Demand))+
  labs(y = "Electricity Consumption",
       x = "Max Temperature of the Day")+
  geom_point() +
  geom_smooth(method = "lm", se = FALSE)

# higher temperatures mean more electricity use for AC

# Qn 1b

vic_elec_fit <- jan14_vic_elec %>%
  model(TSLM(Demand ~ Temperature))

vic_elec_fit %>% report()

# residual diagnostics
vic_elec_fit %>% gg_tsresiduals()

# residual plots against predictors (7.3)
# check out the book for facet wrap and putting on multiple predictors

jan14_vic_elec %>% 
  left_join(residuals(vic_elec_fit), by = "Date") %>%
  ggplot(aes(x = Temperature, y = .resid)) +
  geom_point() +
  labs(y = "Residuals", x = "")

# residual plots against fitted values

augment(vic_elec_fit) %>%
  ggplot(aes(x = .fitted, y = .resid)) +
  geom_point() +
  labs(x = "Fitted",
       y = "Residuals")

# this is the same as the previous graph because we only have 1 predictor.
# no outliers or influential observations
# residuals are far from normally distributed. 
# Many high negative residuals where the model is highly over predicting
# there is high auto correlation at time 1, 
# and we can see that the residuals seem to follow a pattern over time
# we also see that the model tends to under predict at the extreme ends, 
# and over predict around 30c

find_funs <- function(f) {
  # Returns dataframe with two columns:
    # `package_name`: packages(s) which the function is part of (chr)
    # `builtin_package`:  whether the package comes with standard R (a 'builtin'  package)

  # Arguments:
    # f: name of function for which the package(s) are to be identified.


  if ("tidyverse" %in% rownames(installed.packages()) == FALSE) {
    cat("tidyverse is needed for this function. Please install. Stopping")
    stop()}

  suppressMessages(library(tidyverse))


  # search for help in list of installed packages
  help_installed <- help.search(paste0("^",f,"$"), agrep = FALSE)

  # extract package name from help file
  pckg_hits <- help_installed$matches[,"Package"]

  if (length(pckg_hits) == 0) pckg_hits <- "No_results_found"


  # get list of built-in packages

  pckgs <- installed.packages()  %>% as_tibble
  pckgs %>%
    dplyr::filter(Priority %in% c("base","recommended")) %>%
    dplyr::select(Package) %>%
    distinct -> builtin_pckgs_df

  # check for each element of 'pckg hit' whether its built-in and loaded (via match). Then print results.

  results <- tibble(
    package_name = pckg_hits,
    builtin_pckage = match(pckg_hits, builtin_pckgs_df$Package, nomatch = 0) > 0,
    loaded = match(paste("package:",pckg_hits, sep = ""), search(), nomatch = 0) > 0
  )

  return(results)

}

find_funs("forecast")
find_funs("model")

# Qn 1c, 1d
# head(jan14_vic_elec)
# head(new_data(jan14_vic_elec, 1) %>% mutate(Temperature = 15))


fc_15 <- jan14_vic_elec %>%
  model(TSLM(Demand ~ Temperature)) %>%
  forecast(
    new_data(jan14_vic_elec, 1) %>%
      mutate(Temperature = 15)
  ) 

fc_15 %>% hilo(level = c(80, 95))
autoplot(fc_15, jan14_vic_elec)

fc_35 <- jan14_vic_elec %>%
  model(TSLM(Demand ~ Temperature)) %>%
  forecast(
    new_data(jan14_vic_elec, 1) %>%
      mutate(Temperature = 35)
  ) 

fc_35 %>% hilo(level = c(80, 95))
autoplot(fc_35, jan14_vic_elec)

# not sure how this confidence interval is calculated? can't find it in the chapter... just standard regression forecast errors?
# the 35c is more likely because that was in the historical data. 
# 15c is so far out of the training period that the forecast is unlikely to be accurate
# at that temperature electricity demand might spike due to heating

# Qn 1e
head(vic_elec)

jan14_vic_elec %>%
  ggplot(aes(x = Temperature, y = Demand))+
  labs(y = "Electricity Consumption",
       x = "Max Temperature of the Day")+
  geom_point() +
  geom_smooth(method = "lm", se = FALSE)

all_vic_elec <- vic_elec %>%
  index_by(Date = as_date(Time)) %>%
  summarise(
    Demand = sum(Demand),
    Temperature = max(Temperature)
  )

all_vic_elec %>%
  ggplot(aes(x = Temperature, y = Demand))+
  labs(y = "Electricity Consumption",
       x = "Max Temperature of the Day")+
  geom_point() +
  geom_smooth(method = "lm", se = FALSE)

# as expected, our model gave a prediction that was inaccurate 
# demand is not a linear function with temperature

"""# Qn 2

Data set olympic_running contains the winning times (in seconds) in each Olympic Games sprint, middle-distance and long-distance track events from 1896 to 2016.

## Qn 2a

Plot the winning time against the year for each event. Describe the main features of the plot.
"""

#exploratory
# head(olympic_running)

# olympic_running %>%
#   distinct(Length, Sex)

olympic_running %>%
  # filter(Sex == "men") %>% 
  filter(!is.na(Time)) %>%
  ggplot(aes(x = Year, y = Time, color=Sex)) + 
    geom_point() +
    scale_color_manual(values = c("blue", "red"))+
    facet_wrap( ~Length, scales="free") +
    geom_smooth(method = "lm", se = FALSE)

"""## Qn 2b

Fit a regression line to the data for each event. Obviously the winning times have been decreasing, but at what average rate per year?
"""

# You can just mod

fit_running <- olympic_running %>%
  model(tslm = TSLM(Time ~ Year))

# glance(fit_running)

coefficients(fit_running)

# testing code

# fc_running <- fit_running %>%
  # forecast(h=5)

# head(fc_running)

"""# Qn 2c

Plot the residuals against the year. What does this indicate about the suitability of the fitted lines?
"""

# fit_running %>% gg_tsresiduals()
fit_running %>% 
  residuals() %>%
  head


fit_running %>% 
  residuals() %>%
  filter(!is.na(.resid)) %>%
  ggplot(aes(x = Year, y = .resid, color=Sex)) + 
    geom_point() +
    scale_color_manual(values = c("blue", "red"))+
    facet_wrap( ~ Length, scales="free_y")
    # geom_smooth(method = "lm", se = FALSE)

# residuals seem to be consistently > 0 in the more recent time period
# this indcates that the model is under-predicting the times
# which makes sense, because we'd expect the linear decrease in times to level off at some point

"""## Qn 2d

Predict the winning time for each race in the 2020 Olympics. Give a prediction interval for your forecasts. What assumptions have you made in these calculations?
"""

# head(olympic_running)

# olympic_running %>%
#   distinct(Year)


fit_running %>%
  forecast()

# assumptions are that the linear trend will continue
# very likely that these forecasts are too optimistic

"""# Qn 4

The data set souvenirs concerns the monthly sales figures of a shop which opened in January 1987 and sells gifts, souvenirs, and novelties. The shop is situated on the wharf at a beach resort town in Queensland, Australia. The sales volume varies with the seasonal population of tourists. There is a large influx of visitors to the town at Christmas and for the local surfing festival, held every March since 1988. Over time, the shop has expanded its premises, range of products, and staff.

## Qn 4a

Produce a time plot of the data and describe the patterns in the graph. Identify any unusual or unexpected fluctuations in the time series.

## 4b

Explain why it is necessary to take logarithms of these data before fitting a model.
"""

head(souvenirs)

souvenirs %>% autoplot(Sales)

# there's a drop in the 1991 christmas peak, and significant increases after that

# logs are necessary because the variance grows with the level
# and the heteroscedasticity makes our tests and CI's invalid

souv_mod <- souvenirs %>%
  # mutate(log_sales = log(Sales)) %>%
  mutate(actual_month = month(Month)) %>%
  mutate(surf_fest = (month(Month)==3) & (Month >= yearmonth("1988 Mar")))

# head(souv_mod)
# souv_mod %>% filter(surf_fest) %>% head()
# souv_mod %>% distinct(surf_fest)

fit_souv <- souv_mod %>%
  model(tslm = TSLM(log(Sales) ~ surf_fest + trend() + season()))

fit_souv %>% report()

fit_souv %>% gg_tsresiduals()

# there is significant auto correlation of the residuals

fit_souv %>% 
  residuals() %>% 
  ggplot(aes(x=as.factor(month(Month)), y=.resid)) + 
    geom_boxplot(fill="slateblue", alpha=0.2) + 
    xlab("Month")
  

# yes there is a problem, the variability of the residuals vary by month, 
# which means there's extra signal we could be capturing

fit_souv %>% report()

# there's sginificant correlation between month 3 and the surf fest
# Dec is the busiest month, almost double of Jan
# There is an increase of 2% per yr

augment(fit_souv) %>% 
  features(.innov, ljung_box, lag=24, dof=14) 

# ljung box text recommends 10 for non seasonal, 2m for seasonal, 
# and T/5 if 2m is too large
# there are 14 estimated parameters
# p value is very low so there is clear auto correlation going on

# souv_mod 
souv_new <- new_data(souv_mod, 3*12) %>%
  mutate(surf_fest = (month(Month)==3) & (Month >= yearmonth("1988 Mar")))


fc_souv <- fit_souv %>%
  forecast(
    new_data = souv_new
  )

fc_souv %>% autoplot() +
  autolayer(souv_mod, Sales)

# to improve the model, include lagged values of the variable in there?
# or use an ARIMA model

"""# Qn 5

The us_gasoline series consists of weekly data for supplies of US finished motor gasoline product, from 2 February 1991 to 20 January 2017. The units are in “million barrels per day”. Consider only the data to the end of 2004.
"""

us_gas <- us_gasoline %>%
  filter(year(Week)<=2004) 

# us_gas %>% autoplot(Barrels)

# max number of fourier terms is K = m/2, 
# which gives us the same as including the seasonal variable

gas_fit <- us_gas %>%
  model(
    TSLM(Barrels ~ trend() + fourier(K = 26))
  )

augment(gas_fit) %>% 
  ggplot(aes(x = Week)) +
  geom_line(aes(y = .fitted, colour = "Fitted")) +
  geom_line(aes(y = Barrels, colour = "Data")) +
  scale_colour_manual(values=c(Data="black",Fitted="#D55E00")) +
  guides(colour = guide_legend(title = NULL))

gas_fit_multi <- us_gas %>%
  model(
    f_26 = TSLM(Barrels ~ trend() + fourier(K = 26)),
    f_13 = TSLM(Barrels ~ trend() + fourier(K = 13)),
    f_06 = TSLM(Barrels ~ trend() + fourier(K = 6))
  )

# augment(gas_fit_multi)

augment(gas_fit_multi) %>% 
  ggplot(aes(x = Week, y=.fitted)) +
    geom_line() +
    facet_grid( .model ~ .)
  
# less fourier terms mean smother seasonality

gas_fit_multi <- us_gas %>%
  model(
    f_26 = TSLM(Barrels ~ trend() + fourier(K = 26)),
    f_13 = TSLM(Barrels ~ trend() + fourier(K = 13)),
    f_08 = TSLM(Barrels ~ trend() + fourier(K = 8)),
    f_07 = TSLM(Barrels ~ trend() + fourier(K = 7)),
    f_06 = TSLM(Barrels ~ trend() + fourier(K = 6)),
    f_05 = TSLM(Barrels ~ trend() + fourier(K = 5)),
    f_04 = TSLM(Barrels ~ trend() + fourier(K = 4)),
    f_03 = TSLM(Barrels ~ trend() + fourier(K = 3)),
    f_02 = TSLM(Barrels ~ trend() + fourier(K = 2)),
    f_01 = TSLM(Barrels ~ trend() + fourier(K = 1)),
  )

gas_fit_multi %>% glance()

# looks like 7 terms is the sweet splot

gas_fit_final <- us_gas %>%
  model(TSLM(Barrels ~ trend() + fourier(K = 7))) 

gas_fit_final %>% gg_tsresiduals()

# residuals look good mostly. 
# There's two autocorrelations that go slightly above the crit value

augment(gas_fit_final) %>% 
  features(.innov, ljung_box, lag=104, dof=16)

# dof is 16 because 2*7 fourier terms, intercept, and trend 
# there is still residual auto correlation

gas_fc <- gas_fit_final %>%
  forecast(h="1 year")

gas_fc %>% autoplot() +
  autolayer(
    us_gasoline %>%
      filter(year(Week) == 2005)
    , Barrels
  )

# forecasts are pretty good. Theres some noise around our forecast
# there is a significant dip around week 38 which our forecast is not capturing

"""# Qn 6

The annual population of Afghanistan is available in the global_economy data set.
"""

afg <- global_economy %>%
  filter(Country == "Afghanistan")

afg %>% autoplot(Population/1e6)

# yes there is significant population increases, 
# and the effects of the war can clearly be seen

afg_fit <- afg %>%
  model(
    linear = TSLM(Population ~ trend()),
    piecewise = TSLM(Population ~ trend(knots = c(1980, 1989)))
  )

# augment(afg_fit)

augment(afg_fit) %>% ggplot(aes(x=Year))+
  geom_line(aes(y=Population)) +
  geom_line(aes(y=.fitted, color=.model))

# the piecewise linear fits much better the non-linear trend in the data
# the linear trend assumes the drop will continue into the future

afg_fc <- forecast(afg_fit, h=5)

afg_fc %>% autoplot() + 
  autolayer(afg, Population)


# the piecewise trend predicts the future much better